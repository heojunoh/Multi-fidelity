import GPy
import numpy as np
from matplotlib import pyplot as plt
import matplotlib.mlab as ml
import matplotlib.patches as mpatches
import scipy.stats as stats

import time

l2error= []
meanscore= []
meancrps= []
comptime= []

for kk in range(1, 101): # 63; SVD did not converge
 np.random.seed(kk)
 
 def high(x):
    x1 = x[:,0]
    x2 = x[:,1]
    return np.sin(2*np.pi*(medium(x)-1))

 def medium(x):
    x1 = x[:,0]
    x2 = x[:,1]
    return np.exp(-1.4*low(x))*np.cos(3.5*np.pi*low(x))

 def low(x):
    x1 = x[:,0]
    x2 = x[:,1]
    return 0.75 * np.exp(-(9*x1-2)**2/4 - (9*x2-2)**2/4) + 0.75 * np.exp(-(9*x1+1)**2/49 - (9*x2+1)/10) + 0.5 * np.exp(-(9*x1-7)**2/4 - (9*x2-3)**2/4) - 0.2 * np.exp(-(9*x1-4)**2 - (9*x2-7)**2) 

 def scale_range(x,ub,lb):
    Np = x.shape[0]
    dim = x.shape[1]
    for i in range(0,Np):
        for j in range(0,dim):
            tmp = ub[j] -lb[j]
            x[i][j] = tmp*x[i][j] + lb[j]
    return x

 def rmse(pred, truth):
    pred = pred.flatten()
    truth = truth.flatten()
    return np.sqrt(np.mean((pred-truth)**2))


 ''' Create training set '''
 N1 = 20
 N2 = 15
 N3 = 10

 plot = 1
 save = 0

 dim = 2
 lb = np.array([0.0, 0.0])
 ub = np.array([1.0, 1.0])

 tmp = np.random.rand(1000,dim)
 Xtrain = scale_range(tmp,ub,lb)
 idx = np.random.permutation(1000)
 X1 = Xtrain[idx[0:N1], :]
 X2 = Xtrain[idx[0:N2], :]
 X3 = Xtrain[idx[0:N3], :]

 Y1 = low(X1)[:,None]
 Y2 = medium(X2)[:,None]
 Y3 = high(X3)[:,None]

 # nn = 40
 # lb = np.array([0.0, 1.0])
 # ub = np.array([0.0, 1.0])
 # x1 = np.linspace(lb[0], ub[0], 50)
 # x2 = np.linspace(lb[1], ub[1], 50)
 # X, Y = np.meshgrid(x1, x2)

 tmp = np.random.rand(1000,2)
 Xtest = scale_range(tmp,ub,lb)

 Exact = high(Xtest)
 Medium = medium(Xtest)
 Low = low(Xtest)

 active_dimensions = np.arange(0,dim)

 start = time.time()

 ''' Train level 1 '''
 k1 = GPy.kern.RBF(dim, ARD = True)
 m1 = GPy.models.GPRegression(X=X1, Y=Y1, kernel=k1)

 m1[".*Gaussian_noise"] = m1.Y.var()*0.01
 m1[".*Gaussian_noise"].fix()

 m1.optimize(max_iters = 500)

 m1[".*Gaussian_noise"].unfix()
 m1[".*Gaussian_noise"].constrain_positive()

 m1.optimize_restarts(30, optimizer = "bfgs",  max_iters = 1000)

 mu1, v1 = m1.predict(X2)


 ''' Train level 2 '''
 XX = np.hstack((X2, mu1))

 k2 = GPy.kern.RBF(1, active_dims = [dim])*GPy.kern.RBF(dim, active_dims = active_dimensions, ARD = True) \
     + GPy.kern.RBF(dim, active_dims = active_dimensions, ARD = True)

 m2 = GPy.models.GPRegression(X=XX, Y=Y2, kernel=k2)

 m2[".*Gaussian_noise"] = m2.Y.var()*0.01
 m2[".*Gaussian_noise"].fix()

 m2.optimize(max_iters = 500)

 m2[".*Gaussian_noise"].unfix()
 m2[".*Gaussian_noise"].constrain_positive()

 m2.optimize_restarts(30, optimizer = "bfgs",  max_iters = 1000)


 # Prepare for level 3: sample f_1 at X3
 nsamples = 100
 ntest = X3.shape[0]
 mu0, C0 = m1.predict(X3, full_cov=True)
 Z = np.random.multivariate_normal(mu0.flatten(),C0,nsamples)
 tmp_m = np.zeros((nsamples,ntest))
 tmp_v = np.zeros((nsamples,ntest))

 # push samples through f_2
 for i in range(0,nsamples):
    mu, v = m2.predict(np.hstack((X3, Z[i,:][:,None])))
    tmp_m[i,:] = mu.flatten()
    tmp_v[i,:] = v.flatten()

 # get mean and variance at X3
 mu2 = np.mean(tmp_m, axis = 0)
 v2 = np.mean(tmp_v, axis = 0) + np.var(tmp_m, axis = 0)
 mu2 = mu2[:,None]
 v3 = np.abs(v2[:,None])


 ''' Train level 3 '''
 XX = np.hstack((X3, mu2))

 k3 = GPy.kern.RBF(1, active_dims = [dim])*GPy.kern.RBF(dim, active_dims = active_dimensions, ARD = True) \
     + GPy.kern.RBF(dim, active_dims = active_dimensions, ARD = True)

 m3 = GPy.models.GPRegression(X=XX, Y=Y3, kernel=k3)

 m3[".*Gaussian_noise"] = m3.Y.var()*0.01
 m3[".*Gaussian_noise"].fix()

 m3.optimize(max_iters = 500)

 m3[".*Gaussian_noise"].unfix()
 m3[".*Gaussian_noise"].constrain_positive()

 m3.optimize_restarts(30, optimizer = "bfgs",  max_iters = 1000)


 # Compute posterior mean and variance for level 3 evaluated at the test points

 # sample f_1 at Xtest
 nsamples = 100
 ntest = Xtest.shape[0]
 mu0, C0 = m1.predict(Xtest, full_cov=True)
 Z = np.random.multivariate_normal(mu0.flatten(),C0,nsamples)

 # push samples through f_2 and f_3
 tmp_m = np.zeros((nsamples**2,ntest))
 tmp_v = np.zeros((nsamples**2,ntest))
 cnt = 0
 for i in range(0,nsamples):
    mu, C = m2.predict(np.hstack((Xtest, Z[i,:][:,None])), full_cov=True)
    Q = np.random.multivariate_normal(mu.flatten(),C,nsamples)
    for j in range(0,nsamples):
        mu, v = m3.predict(np.hstack((Xtest, Q[j,:][:,None])))
        tmp_m[cnt,:] = mu.flatten()
        tmp_v[cnt,:] = v.flatten()
        cnt = cnt + 1


 # get f_2 posterior mean and variance at Xtest
 mu3 = np.mean(tmp_m, axis = 0)
 v3 = np.mean(tmp_v, axis = 0) + np.var(tmp_m, axis = 0)
 mu3 = mu3[:,None]
 v3 = np.abs(v3[:,None])

 end = time.time()


 Exact = Exact[:,None]

 error = np.sqrt(np.mean((mu3-Exact)**2))
 score = np.mean(-(Exact-mu3)**2/v3-np.log(v3))
 crps = np.mean(-np.sqrt(v3)*(1/np.sqrt(np.pi)-2*stats.norm.pdf((Exact-mu3)/np.sqrt(v3))-(Exact-mu3)/np.sqrt(v3)*(2*stats.norm.cdf((Exact-mu3)/np.sqrt(v3))-1)))
 ctime = (end - start)
 # print( "N1 = %d, N2 = %d, sample = %d, error = %e" % (N1, N2[ii], jj+1, error))

 l2error.append(error)
 meanscore.append(score)
 meancrps.append(crps)
 comptime.append(ctime)
 

l2error
np.mean(l2error) 
np.sort(l2error) 

meanscore 
np.mean(meanscore) 
np.sort(meanscore) 

meancrps
np.mean(meancrps) 
np.sort(meancrps) 

comptime



### RMSE ### 
c(0.5511443885324832, 0.6596750505439009, 0.6849074620635947, 0.6068789900905118, 0.9611439840771514, 
0.7710328723117196, 0.6444712743740411, 0.7431999660089059, 0.7845206726961997, 0.5759764789667287, 
0.5346417945939718, 0.7071543256872981, 0.6118137770298608, 0.7577922060166076, 0.7878397684517019, 
0.7666807626976353, 0.5421965681062701, 0.545833991058973, 0.7638972293390064, 0.7819956555524649, 
0.6895415085362098, 0.6316336219290457, 0.7331878573017473, 0.5473299706700595, 0.645152935075807, 
0.658652708707487, 0.7269947804708726, 0.7317881940229867, 0.7159739581611791, 0.7330850934213795, 
0.7629645399398459, 0.6688970841603887, 0.8547300123428879, 0.6206954813612146, 0.6089246770888753, 
0.5704676662722323, 0.6386987795868743, 0.6394497492724655, 0.6203402216163288, 0.86373560213365, 
0.6960460619649649, 0.7757263310792275, 0.5804835363241708, 0.6694584145628527, 0.6164537199395331, 
0.6760912580346445, 0.6829915842859478, 0.6163662085979954, 0.6472182543008344, 0.7451274508814558, 
0.553385608420307, 0.5985391204385572, 0.6725486911652034, 0.6915490689913442, 0.6725604208633503, 
0.5500150389853841, 0.7048326743971403, 0.6316724808981606, 0.6366837699955432, 0.5592483178288905, 
0.6413031581504399, 0.6413031581504399, 0.7161672309296464, 0.5684778574701264, 0.7972733318813475, 
0.6592057639291572, 0.5411544077755798, 0.5616109509795045, 0.5909782876115647, 0.7043338395691602, 
0.554034980033292, 0.5692825068446294, 0.6426497978864342, 0.5667769778770291, 0.6797704289124331, 
0.5266794608367036, 0.7126113752623242, 0.7564715599685653, 0.630750626680051, 0.5950322307473801, 
0.7824573608363246, 0.6438479624136744, 0.8287452873603723, 0.630105545550783, 0.5581817122963557, 
0.72566043402063, 0.7098667663519718, 0.6224230110470849, 0.6830183107679724, 0.5528958709325407, 
0.49071890796148643, 0.6587663151066209, 0.5562792642491277, 0.7542139774838275, 0.6017206228079495, 
0.6669038394139393, 0.7459977160419697, 0.6733708865506871, 0.6501331492665351, 0.7206131698068257)

### mean CRPS result.branin.meancrps ### The smaller, the better
c(0.28897173135762844, 0.38001987647033136, 0.3901871913076911, 0.3357667552290378, 0.6264033853526362, 
0.4529880578354316, 0.37249206921637074, 0.4369112078310833, 0.46669746029992437, 0.30873200355816544, 
0.271851155047444, 0.4117122740247307, 0.33595121771805747, 0.4643573793720561, 0.47851924121490436, 
0.4378370647683306, 0.2930681478049687, 0.2760983591543816, 0.44896803838417826, 0.43619765767098767, 
0.40120302561334636, 0.35660728459283403, 0.44093907597107945, 0.2948376108011204, 0.35395133930696315, 
0.3671513201565945, 0.4215738457264353, 0.4272297102921997, 0.41812540499164264, 0.42769246404724426, 
0.4794921021578388, 0.35065196224582773, 0.5286052847680187, 0.33986295354986046, 0.31850489724131875, 
0.3023930642857322, 0.3453335292423643, 0.3494420314138799, 0.34152032294634777, 0.5693658970144243, 
0.40025192749570815, 0.4576638706000773, 0.3068501793036992, 0.3816272007445675, 0.3269210695917484, 
0.3792022791785919, 0.3945377293233353, 0.3466095127194851, 0.3579534395119872, 0.43948083088108875, 
0.28396416946041725, 0.3282977689523598, 0.3793415474288098, 0.40008890731927227, 0.3902492069684566, 
0.2859709903516653, 0.4124655131866682, 0.3399022895697458, 0.3479461745272443, 0.28775951779433645, 
0.3592086329002883, 0.3592086329002883, 0.4227412534088286, 0.30679821168632987, 0.5090249013225981, 
0.36806014901993556, 0.28371676325676054, 0.28360807883858463, 0.3132943576721815, 0.4019755267308435, 
0.2841378338294835, 0.3018195349438574, 0.3526153178360991, 0.2971083951586694, 0.3832082712950782, 
0.2808789198484714, 0.4282899995226224, 0.4438494715903681, 0.34094704751487814, 0.31135270388383013, 
0.45026819296648113, 0.36120073045966944, 0.5036575037944042, 0.3392850145050295, 0.30108261551912213, 
0.4153547538434583, 0.4168722046483455, 0.34997293150970393, 0.39011951821543445, 0.29832171560471216, 
0.25423372078041456, 0.36628755380871325, 0.2936671677835252, 0.4555573892485559, 0.32913400796557085, 
0.3857929357877079, 0.42711877998828196, 0.35034509792775165, 0.34785563483732046, 0.4205077288360665)

### computation time result.branin.comptime ### The smaller, the better
c(131.58816719055176, 193.98071193695068, 175.7153582572937, 196.55459213256836, 189.7441108226776, 
180.15572500228882, 190.9534330368042, 167.68277502059937, 178.62781882286072, 159.25829005241394, 
172.88579297065735, 212.43094873428345, 175.0363998413086, 182.27984929084778, 176.92628717422485, 
183.5248727798462, 184.2406063079834, 165.13108801841736, 213.01585602760315, 196.10092306137085, 
171.23573780059814, 184.95077896118164, 187.24046802520752, 184.54044795036316, 151.77827095985413, 
167.4719729423523, 168.96429085731506, 170.85820078849792, 176.580904006958, 207.66964411735535, 
156.30769085884094, 181.0253129005432, 169.366192817688, 142.02578592300415, 151.27464389801025, 
139.34179711341858, 138.86328291893005, 147.94982600212097, 175.71044993400574, 155.05779314041138, 
156.02310585975647, 175.8110179901123, 198.23550510406494, 183.66553497314453, 156.96297192573547, 
170.78364396095276, 189.10905170440674, 184.97563695907593, 173.18184113502502, 187.47554183006287, 
200.18465995788574, 182.57795095443726, 133.4921441078186, 140.89774918556213, 150.2748682498932, 
130.7517387866974, 151.8335099220276, 130.1663429737091, 140.09835171699524, 137.67639589309692, 
176.2342598438263, 140.63138008117676, 114.55709886550903, 111.80307674407959, 131.57559084892273, 
98.31709218025208, 99.95511603355408, 107.54872965812683, 105.12071967124939, 127.22969818115234, 
105.37112188339233, 98.97530913352966, 96.53790712356567, 99.96590375900269, 122.99387812614441, 
105.3841028213501, 113.64740800857544, 111.2693121433258, 98.93330216407776, 99.08079481124878, 
113.4723129272461, 113.66482305526733, 122.41466021537781, 102.70020604133606, 113.67346811294556, 
120.50010991096497, 108.61943507194519, 111.11429405212402, 110.45885372161865, 100.26177430152893, 
101.47745490074158, 102.3727560043335, 101.48106694221497, 118.30065703392029, 109.76288890838623, 
98.3718409538269, 127.53809475898743, 105.30002117156982, 99.86162495613098, 98.55333828926086)


